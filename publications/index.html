<html lang="en"><head>
    <meta charset="UTF-8">
    <title>Research</title>
    <link rel="icon" type="image/x-icon" href="https://avatars.githubusercontent.com/zouchuhang"/>
<style id="system" type="text/css">body{}</style><style id="custom" type="text/css">html { font-size: 100%; overflow-y: scroll; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; }

body{
color:#444;
font-family:Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
font-size:12px;
line-height:1.5em;
padding:1em;
margin:auto;
max-width:55em;
background:#fefefe;
}

a{ color: #0645ad; text-decoration:none;}
a:visited{ color: #0b0080; }
a:hover{ color: #06e; }
a:active{ color:#faa700; }
a:focus{ outline: thin dotted; }
a:hover, a:active{ outline: 0; }

::-moz-selection{background:rgba(255,255,0,0.3);color:#000}
::selection{background:rgba(255,255,0,0.3);color:#000}

a::-moz-selection{background:rgba(255,255,0,0.3);color:#0645ad}
a::selection{background:rgba(255,255,0,0.3);color:#0645ad}

p{
margin:1em 0;
}

img{
max-width:100%;
}

h1,h2,h3,h4,h5,h6{
font-weight:normal;
color:#111;
line-height:1em;
}
h4,h5,h6{ font-weight: bold; }
h1{ font-size:2.5em; }
h2{ font-size:2em; }
h3{ font-size:1.5em; }
h4{ font-size:1.2em; }
h5{ font-size:1em; }
h6{ font-size:0.9em; }

blockquote{
color:#666666;
margin:0;
padding-left: 3em;
border-left: 0.5em #EEE solid;
}
hr { display: block; height: 2px; border: 0; border-top: 1px solid #aaa;border-bottom: 1px solid #eee; margin: 1em 0; padding: 0; }
pre, code, kbd, samp { color: #000; font-family: monospace, monospace; _font-family: 'courier new', monospace; font-size: 0.98em; }
pre { white-space: pre; white-space: pre-wrap; word-wrap: break-word; }

b, strong { font-weight: bold; }

dfn { font-style: italic; }

ins { background: #ff9; color: #000; text-decoration: none; }

mark { background: #ff0; color: #000; font-style: italic; font-weight: bold; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }
sup { top: -0.5em; }
sub { bottom: -0.25em; }

ul, ol { margin: 1em 0; padding: 0 0 0 2em; }
/*li p:last-child { margin:0 }*/
li p:last-child { margin:0.8em }
dd { margin: 0 0 0 2em; }

img { border: 0; -ms-interpolation-mode: bicubic; vertical-align: middle; }

table { border-collapse: collapse; border-spacing: 0; }
td { vertical-align: top; }

@media only screen and (min-width: 480px) {
body{font-size:14px;}
}

@media only screen and (min-width: 768px) {
body{font-size:16px;}
}

@media print {
  * { background: transparent !important; color: black !important; filter:none !important; -ms-filter: none !important; }
  body{font-size:12pt; max-width:100%;}
  a, a:visited { text-decoration: underline; }
  hr { height: 1px; border:0; border-bottom:1px solid black; }
  a[href]:after { content: " (" attr(href) ")"; }
  abbr[title]:after { content: " (" attr(title) ")"; }
  .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after { content: ""; }
  pre, blockquote { border: 1px solid #999; padding-right: 1em; page-break-inside: avoid; }
  tr, img { page-break-inside: avoid; }
  img { max-width: 100% !important; }
  @page :left { margin: 15mm 20mm 15mm 10mm; }
  @page :right { margin: 15mm 10mm 15mm 20mm; }
  p, h2, h3 { orphans: 3; widows: 3; }
  h2, h3 { page-break-after: avoid; }
}
</style></head>

<p align=right>【<strong><a href="https://zouchuhang.github.io/">HOME</a></strong>】<!--&nbsp;&nbsp; 【<strong><a href="https://www.linkedin.com/in/yijun-li-1117a9132/">LINKEDIN</a></strong>】-->
</p>

<hr>
<body marginheight="0"><h1>Publications</h1>
<p><a href="https://scholar.google.com/citations?user=jBcKes0AAAAJ&hl=en">(Google scholar)</a></p>
<hr>

<p>
 <img src="quickview/fluid_2025.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time</font></span>
 <br>
 <span class="ban2"> &nbsp; Jingxuan Xu, Hong Huang, <b>Chuhang Zou</b>, Manolis Savva, Yunchao Wei and Wuyang Chen</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2025</span>
 <br>
<span class="ban2"> &nbsp; <a href="https://hybridmpm.github.io/">Website</a>&nbsp; <a href="https://arxiv.org/pdf/2505.18926">arxiv</a>&nbsp;</span>
 <br>
</p>
    
<p><br></p>

<p>
 <img src="quickview/mvbench.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">MVGBench: a Comprehensive Benchmark for Multi-view Generation Models</font></span>
 <br>
 <span class="ban2"> &nbsp; Xianghui Xie, <b>Chuhang Zou</b>, Meher Gitika Karumuri, Jan Eric Lenssen and Gerard Pons-Moll</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2025</span>
 <br>
<span class="ban2"> &nbsp; <a href="https://virtualhumans.mpi-inf.mpg.de/MVGBench/">Website</a>&nbsp; <a href="https://virtualhumans.mpi-inf.mpg.de/MVGBench/MVGBench.pdf">arxiv</a>&nbsp;</span>
 <br>
</p>
    
<p><br></p>

<p>
 <img src="quickview/EasyDream.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Direct and Explicit 3D Generation from a Single Image</font></span>
 <br>
    <span class="ban2"> &nbsp; <font color="red"><b>Amazon's First 3D Foundation Model in Use!</b></font> </span>
 <br>
 <span class="ban2"> &nbsp; Haoyu Wu, Meher Gitika Karumuri, <b>Chuhang Zou</b>, Seungbae Bang, Yuelong Li, Dimitris Samaras and Sunil Hadap</span>
 <br>
 <span class="ban2"> &nbsp; International Conference on 3D Vision (<b>3DV</b>), 2025</span>
 <br>
<span class="ban2"> &nbsp; <a href="https://hao-yu-wu.github.io/gen3d/">Website</a> &nbsp; <a href="https://arxiv.org/pdf/2411.10947">Paper</a>&nbsp; </span>
 <br>
</p>
    
<p><br></p>

<p>
 <img src="quickview/MonoPatchNerf.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">MonoPatchNeRF: Improving Neural Radiance Fields with Patch-based Monocular Guidance</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Yuqun Wu, Jae Yong Lee, <b>Chuhang Zou</b>, Shenlong Wang, and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; International Conference on 3D Vision (<b>3DV</b>), 2025</span>
 <br>
<span class="ban2"> &nbsp; <a href="https://yuqunw.github.io/MonoPatchNeRF/">Website</a> &nbsp; <a href="https://arxiv.org/pdf/2404.08252.pdf">Paper</a>&nbsp; <a href="https://github.com/yuqunw/monopatch_nerf">Code</a>&nbsp;</span>
 <br>
</p>
    
<p><br></p>

<p>
 <img src="quickview/PPNG.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Plenoptic PNG: Real-Time Neural Radiance Fields in 150 KB</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Yuqun Wu, Jae Yong Lee, <b>Chuhang Zou</b>, Derek Hoiem and Shenlong Wang</span>
 <br>
 <span class="ban2"> &nbsp; International Conference on 3D Vision (<b>3DV</b>), 2025</span>
 <br>
<span class="ban2"> &nbsp; <a href="https://jyl.kr/ppng/">Website</a> &nbsp; <a href="https://arxiv.org/pdf/2409.15689">Paper</a>&nbsp; <a a href="https://github.com/leejaeyong7/ppng">Code</a></span>
 <br>
</p>
    
<p><br></p>

<p>
 <img src="quickview/ipm.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Ordinal focal loss: A relative fashionability ranking learning method</font></span>
 <br>
 <span class="ban2"> &nbsp; Ling Ma, <b>Chuhang Zou</b>, Ziyi Guo, Tao Li, Zheng Liu and Fengyuan Zou</span>
 <br>
 <span class="ban2"> &nbsp; Information Processing & Management (IPM), 2025</span>
 <br>
<span class="ban2"> &nbsp; <a href="https://www.sciencedirect.com/science/article/abs/pii/S0306457325001463">Paper</a>&nbsp; </span>
 <br>
</p>
    
<p><br></p>
    
<p>
    
 <img src="quickview/T4V.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Recurrent Transformer Variational Autoencoders for Multi-Action Motion Synthesis</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Rania Briq, <b>Chuhang Zou</b>, Leonid Pishchulin, Christopher Broaddus, Jürgen Gall</span>
 <br>
 <span class="ban2"> &nbsp; IEEE Conference on Computer Vision and Pattern Recognition Workshop, 2022.</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/2206.06741.pdf">Paper</a></span>
 <br>
</p>
    
<p><br></p>

<p>    
 <img src="quickview/qff_arxiv.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">QFF: Quantized Fourier Features for Neural Field Representations</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Jae Yong Lee, Yuqun Wu, <b>Chuhang Zou</b>, Shenlong Wang, and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2022</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/2212.00914.pdf">Paper</a>&nbsp; </span>
 <br>
</p>
    
<p><br></p>
    
<p>
 <img src="quickview/PMRL_plusplus.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Deep PatchMatch MVS with Learned Patch Coplanarity, Geometric Consistency and Adaptive Pixel Sampling</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Jae Yong Lee, <b>Chuhang Zou</b> and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2022</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://jyl.kr/patch-match-mvs-net/">Website</a> &nbsp; <a href="https://arxiv.org/pdf/2210.07582.pdf">Paper</a>&nbsp; <a a href="https://github.com/leejaeyong7/patchmatch-rl">Code</a></span>
 <br>
</p>
    
<p><br></p>

 <p>
 <img src="quickview/IJCV_Layout.png" height="190" width="250" align="left"><span> &nbsp; <font size="4">Manhattan Room Layout Reconstruction from a Single 360 image: A Comparative Study of State-of-the-art Methods</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Jheng-Wei Su, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; International Journal of Computer Vision (<b>IJCV</b>), 2021</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/1910.04099.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/LayoutNetv2">Code</a>&nbsp; <a href="https://github.com/ericsujw/Matterport3DLayoutAnnotation">Data</a></span>
 <br>
</p>
    
<p><br></p> 
    
<p>
 <img src="quickview/ICCV_PMRL.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">PatchMatch-RL: Deep MVS with Pixelwise Depth, Normal, and Visibility</font></span>
 <br>
    <span class="ban2"> &nbsp; <font color="red"><b>Oral</b></font> </span>
 <br>
 <span class="ban2"> &nbsp; Jae Yong Lee, Joseph DeGol, <b>Chuhang Zou</b> and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; IEEE International Conference on Computer Vision (<b>ICCV</b>), 2021</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://jyl.kr/patch-match-mvs-net/">Website</a> &nbsp; <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_PatchMatch-RL_Deep_MVS_With_Pixelwise_Depth_Normal_and_Visibility_ICCV_2021_paper.pdf">Paper</a>&nbsp; <a a href="https://github.com/leejaeyong7/patchmatch-rl">Code</a></span>
 <br>
</p>
    
<p><br></p>

<p>
 <img src="quickview/multi_task.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Multi-Task Learning from Videos via Efficient Inter-Frame Attention</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Donghyun Kim, Lan Tian, <b>Chuhang Zou</b>, Ning Xu, Bryan A. Plummer, Stan Sclaroff, Jayan Eledath, Gerard Medioni</span>
 <br>
 <span class="ban2"> &nbsp; IEEE International Conference on Computer Vision Workshop, 2021.</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://openaccess.thecvf.com/content/ICCV2021W/DeepMTL/papers/Kim_MILA_Multi-Task_Learning_From_Videos_via_Efficient_Inter-Frame_Attention_ICCVW_2021_paper.pdf">Paper</a></span>
 <br>
</p>
    
<p><br></p>
    
 <p>
  
 <img src="quickview/wacv_silhouette.png" height="140" width="250" align="left"><span> &nbsp; <font size="4">Silhouette Guided Point Cloud Reconstruction beyond Occlusion</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b> and Derek Hoiem</span>
 <br>
    <span class="ban2"> &nbsp; Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2020</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Zou_Silhouette_Guided_Point_Cloud_Reconstruction_beyond_Occlusion_WACV_2020_paper.pdf">Paper</a> &nbsp; <a href="https://arxiv.org/pdf/1907.12253.pdf">arxiv</a> &nbsp; <a href="https://github.com/zouchuhang/Silhouette-Guided-3D">Code</a></span>
 <br>
</p>
    
<p><br></p>
    
<p>
 <img src="quickview/style.png" height="120" width="250" align="left"><span> &nbsp; <font size="4">Improving Style Transfer with Calibrated Metrics</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Mao-Chuang Yeh, Shuai Tang, Anand Bhattad, <b>Chuhang Zou</b> and David Forsyth</span>
 <br>
 <span class="ban2"> &nbsp; Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2020</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://openaccess.thecvf.com/content_WACV_2020/papers/Yeh_Improving_Style_Transfer_with_Calibrated_Metrics_WACV_2020_paper.pdf">Paper</a> </span> &nbsp;<a href="https://github.com/stringtron/quantative_style">Code</a> </span>
 <br>
</p>

<p><br></p>

<!--
<p>
<img src="quickview/uiuc_c.png" height="120" width="250" align="left">
 <span> &nbsp; <font size="4">3D Scene and Object Parsing from a Single Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b> </span>
 <br>
    <span class="ban2"> &nbsp; Ph.D. Thesis, University of Illinois at Urbana-Champaign, 2019</span>
 <br> 
 <span class="ban2"> &nbsp; <a href="https://www.ideals.illinois.edu/handle/2142/105665">Paper</a> &nbsp;</span>
 <br>
</p>


<p><br></p> -->
    
<p>
<img src="quickview/arxiv_complete.png" height="100" width="250" align="left">
 <span> &nbsp; <font size="4">Complete 3D Scene Parsing from an RGBD image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Ruiqi Guo, Zhizhong Li and Derek Hoiem</span>
 <br>
    <span class="ban2"> &nbsp; International Journal of Computer Visionn (<b>IJCV</b>), 2019</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://link.springer.com/article/10.1007%2Fs11263-018-1133-z">Paper</a> &nbsp; <a href="https://arxiv.org/pdf/1710.09490.pdf">arxiv</a> &nbsp; <a href="https://www.dropbox.com/s/cmjv0dfl7zkzanf/NYUdv2_newgt_cleaned.zip?dl=0">Data</a></span>
 <br>
</p>

<p><br></p>

<p>
 <img src="quickview/iccvw_depth.png" height="120" width="250" align="left"><span> &nbsp; <font size="4">Counterfactual Depth from a Single RGB Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Theerasit Issaranon, <b>Chuhang Zou</b> and David Forsyth</span>
 <br>
 <span class="ban2"> &nbsp; IEEE International Conference on Computer Vision Workshop, 2019</span>
 <br>
 <span class="ban2"> &nbsp; <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/3DRW/Issaranon_Counterfactual_Depth_from_a_Single_RGB_Image_ICCVW_2019_paper.pdf">Paper</a> </span> &nbsp;<a href="https://github.com/Theerasit/CounterfactualDepth">Code</a> </span>
 <br>
</p>

<p><br></p>
    
<p>
 <img src="quickview/CVPR_LayoutNet.png" height="175" width="250" align="left"><span> &nbsp; <font size="4">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Alex Colburn, Qi Shan and Derek Hoiem</span>
 <br>
    <span class="ban2"> &nbsp; IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018</span>
 <br>
 <span class="ban2"> &nbsp; <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0409.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/LayoutNet">Code</a> &nbsp; <a href="https://towardsdatascience.com/the-10-coolest-papers-from-cvpr-2018-11cb48585a49">The 10 coolest papers from CVPR 2018</a> &nbsp; <a href="https://europe.naverlabs.com/Blog/CVPR-2018-Part-4-3D-Scene-Understanding/">Naver Labs Europe Blog</a> &nbsp; <a href="https://www.jiqizhixin.com/articles/2018-04-04-4">Synced AI Technology & Industry Review (Chinese)</a> &nbsp; <a href="https://shiropen.com/seamless/LayoutNet">Seamless (Japanese)</a> &nbsp; <a href="https://www.youtube.com/watch?v=NBxsfyDaPuk&feature=youtu.be">YouTube 3rd Party Intro</a> &nbsp; <a href="https://zhuanlan.zhihu.com/p/130965653?utm_source=wechat_session">ZhiHu</a></span>
 <br>
</p>
    
<p><br></p>
    
<p>
 <img src="quickview/ICCV_3DPRNN.png" height="125" width="250" align="left"><span> &nbsp; <font size="4">3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Ersin Yumer, Jimei Yang, Duygu Ceylan and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; IEEE International Conference on Computer Vision (<b>ICCV</b>), 2017</span>
 <br>
 <span class="ban2"> &nbsp; <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zou_3D-PRNN_Generating_Shape_ICCV_2017_paper.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/3D-PRNN">Code</a> &nbsp; <a href="https://drive.google.com/file/d/19Jvu6urlDnNAG2gZRf-tObTqyRoeLfpm/view?usp=sharing">Data</a> </span>
 <br>
</p>

<p><br></p>    
    
<p>
 <img src="quickview/TRJ.jpeg" height="120" width="250" align="left"><span> &nbsp; <font size="4">Extraction and Classification of She Nationality Clothing via Visual Features</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Xiaojun Ding, <b>Chuhang Zou</b>, Jingyu Chen and Fengyuan Zou</span>
 <br>
    <span class="ban2"> &nbsp; Textile Research Journal (TRJ), 2015</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://journals.sagepub.com/doi/abs/10.1177/0040517515609260?journalCode=trjc">Paper</a></span>
 <br>
</p>

<p><br></p>

<p>
 <img src="quickview/arxiv_scene.png" height="120" width="250" align="left"><span> &nbsp; <font size="4">Predicting Complete 3D Models of Indoor Scene</font></span>
 <br>
 <br>
    <span class="ban2"> &nbsp; Ruiqi Guo, <b>Chuhang Zou</b> and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2015</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/1504.02437.pdf">Paper</a> &nbsp; <a href="https://github.com/arron2003/rgbd2full3d">Code</a></span>
 <br>
</p>
 
<p><br></p>
    
<p>
 <img src="quickview/Saliency.png" height="150" width="250" align="left"><span> &nbsp; <font size="4">Salient Object Detection via Fast Iterative Truncated Nuclear Norm Recovery</font></span>
 <br>
 <span class="ban2"> &nbsp; <font color="red"><b>Full Oral</b></font> </span>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Yao Hu, Deng Cai and Xiaofei He</span>
 <br>
    <span class="ban2"> &nbsp; International Conference on Intelligence Science and Big Data Engineering (ISCIDE), 2013</span>
 <br> 
 <span class="ban2"> &nbsp; <a href="https://link.springer.com/chapter/10.1007/978-3-642-42057-3_31">Paper</a></span>
 <br>
</p>    
    
<p>
<br>
</p>

<hr>
</body></html>
