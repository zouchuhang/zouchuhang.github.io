<html lang="en"><head>
    <meta charset="UTF-8">
    <title>Research</title>
    <link rel="icon" type="image/x-icon" href="https://avatars.githubusercontent.com/zouchuhang"/>
<style id="system" type="text/css">body{}</style><style id="custom" type="text/css">html { font-size: 100%; overflow-y: scroll; -webkit-text-size-adjust: 100%; -ms-text-size-adjust: 100%; }

body{
color:#444;
font-family:Georgia, Palatino, 'Palatino Linotype', Times, 'Times New Roman', serif;
font-size:12px;
line-height:1.5em;
padding:1em;
margin:auto;
max-width:55em;
background:#fefefe;
}

a{ color: #0645ad; text-decoration:none;}
a:visited{ color: #0b0080; }
a:hover{ color: #06e; }
a:active{ color:#faa700; }
a:focus{ outline: thin dotted; }
a:hover, a:active{ outline: 0; }

::-moz-selection{background:rgba(255,255,0,0.3);color:#000}
::selection{background:rgba(255,255,0,0.3);color:#000}

a::-moz-selection{background:rgba(255,255,0,0.3);color:#0645ad}
a::selection{background:rgba(255,255,0,0.3);color:#0645ad}

p{
margin:1em 0;
}

img{
max-width:100%;
}

h1,h2,h3,h4,h5,h6{
font-weight:normal;
color:#111;
line-height:1em;
}
h4,h5,h6{ font-weight: bold; }
h1{ font-size:2.5em; }
h2{ font-size:2em; }
h3{ font-size:1.5em; }
h4{ font-size:1.2em; }
h5{ font-size:1em; }
h6{ font-size:0.9em; }

blockquote{
color:#666666;
margin:0;
padding-left: 3em;
border-left: 0.5em #EEE solid;
}
hr { display: block; height: 2px; border: 0; border-top: 1px solid #aaa;border-bottom: 1px solid #eee; margin: 1em 0; padding: 0; }
pre, code, kbd, samp { color: #000; font-family: monospace, monospace; _font-family: 'courier new', monospace; font-size: 0.98em; }
pre { white-space: pre; white-space: pre-wrap; word-wrap: break-word; }

b, strong { font-weight: bold; }

dfn { font-style: italic; }

ins { background: #ff9; color: #000; text-decoration: none; }

mark { background: #ff0; color: #000; font-style: italic; font-weight: bold; }

sub, sup { font-size: 75%; line-height: 0; position: relative; vertical-align: baseline; }
sup { top: -0.5em; }
sub { bottom: -0.25em; }

ul, ol { margin: 1em 0; padding: 0 0 0 2em; }
/*li p:last-child { margin:0 }*/
li p:last-child { margin:0.8em }
dd { margin: 0 0 0 2em; }

img { border: 0; -ms-interpolation-mode: bicubic; vertical-align: middle; }

table { border-collapse: collapse; border-spacing: 0; }
td { vertical-align: top; }

@media only screen and (min-width: 480px) {
body{font-size:14px;}
}

@media only screen and (min-width: 768px) {
body{font-size:16px;}
}

@media print {
  * { background: transparent !important; color: black !important; filter:none !important; -ms-filter: none !important; }
  body{font-size:12pt; max-width:100%;}
  a, a:visited { text-decoration: underline; }
  hr { height: 1px; border:0; border-bottom:1px solid black; }
  a[href]:after { content: " (" attr(href) ")"; }
  abbr[title]:after { content: " (" attr(title) ")"; }
  .ir a:after, a[href^="javascript:"]:after, a[href^="#"]:after { content: ""; }
  pre, blockquote { border: 1px solid #999; padding-right: 1em; page-break-inside: avoid; }
  tr, img { page-break-inside: avoid; }
  img { max-width: 100% !important; }
  @page :left { margin: 15mm 20mm 15mm 10mm; }
  @page :right { margin: 15mm 10mm 15mm 20mm; }
  p, h2, h3 { orphans: 3; widows: 3; }
  h2, h3 { page-break-after: avoid; }
}
</style></head>

<p align=right>【<strong><a href="https://zouchuhang.github.io/">HOME</a></strong>】<!--&nbsp;&nbsp; 【<strong><a href="https://www.linkedin.com/in/yijun-li-1117a9132/">LINKEDIN</a></strong>】-->
</p>

<hr>
<body marginheight="0"><h1>Publications</h1>
<p><a href="https://scholar.google.com/citations?user=jBcKes0AAAAJ&hl=en">(Google scholar)</a></p>
<hr>
    
<p>
 <img src="quickview/multi_task.png" height="200" width="200" align="left"><span> &nbsp; <font size="4">Multi-Task Learning from Videos via Efficient Inter-Frame Attention</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Donghyun Kim, Tian Lan, <b>Chuhang Zou</b>, Ning Xu, Bryan A. Plummer, Stan Sclaroff, Jayan Eledath, Gerard Medioni</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2020</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/2002.07362.pdf">Paper</a></span>
 <br>
</p>
    
<p><br></p>
    
<p>
 <img src="quickview/arxiv_Layout.png" height="200" width="200" align="left"><span> &nbsp; <font size="4">3D Manhattan Room Layout Reconstruction from a Single 360 Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>*, Jheng-Wei Su*, Chi-Han Peng, Alex Colburn, Qi Shan, Peter Wonka, Hung-Kuo Chu and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2019</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/1910.04099.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/LayoutNetv2">Code</a></span>
 <br>
</p>
    
<p><br></p>
    
 <p>
 <img src="quickview/wacv_silhouette.png" height="170" width="200" align="left"><span> &nbsp; <font size="4">Silhouette Guided Point Cloud Reconstruction beyond Occlusion</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b> and Derek Hoiem</span>
 <br>
    <span class="ban2"> &nbsp; Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2020</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/1907.12253.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/Silhouette-Guided-3D">Code</a></span>
 <br>
</p>
    
<p><br></p>
    
<p>
 <img src="quickview/style.png" height="150" width="200" align="left"><span> &nbsp; <font size="4">Improving Style Transfer with Calibrated Metrics</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Mao-Chuang Yeh, Shuai Tang, Anand Bhattad, <b>Chuhang Zou</b> and David Forsyth</span>
 <br>
 <span class="ban2"> &nbsp; Winter Conference on Applications of Computer Vision (<b>WACV</b>), 2020</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/1910.09447">Paper</a> </span> &nbsp;<a href="https://github.com/stringtron/quantative_style">Code</a> </span>
 <br>
</p>

<p><br></p>

<p>
 <img src="quickview/iccvw_depth.png" height="120" width="200" align="left"><span> &nbsp; <font size="4">Counterfactual Depth from a Single RGB Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Theerasit Issaranon, <b>Chuhang Zou</b> and David Forsyth</span>
 <br>
 <span class="ban2"> &nbsp; IEEE International Conference on Computer Vision Workshop (ICCVW), 2019</span>
 <br>
 <span class="ban2"> &nbsp; <a href="http://openaccess.thecvf.com/content_ICCVW_2019/papers/3DRW/Issaranon_Counterfactual_Depth_from_a_Single_RGB_Image_ICCVW_2019_paper.pdf">Paper</a> </span> &nbsp;<a href="https://github.com/Theerasit/CounterfactualDepth">Code</a> </span>
 <br>
</p>
    

<p><br></p>    
    
<p>
<img src="quickview/uiuc_c.png" height="200" width="200" align="left">
 <span> &nbsp; <font size="4">3D Scene and Object Parsing from a Single Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b> </span>
 <br>
    <span class="ban2"> &nbsp; Ph.D. Thesis</span>
 <br>
    <span class="ban2"> &nbsp; University of Illinois at Urbana-Champaign, 2019</span>
 <br> 
 <span class="ban2"> &nbsp; <a href="https://www.ideals.illinois.edu/handle/2142/105665">Paper</a> &nbsp;</span>
 <br>
</p>

<p><br></p>

<p>
<img src="quickview/arxiv_complete.png" height="100" width="200" align="left">
 <span> &nbsp; <font size="4">Complete 3D Scene Parsing from an RGBD image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Ruiqi Guo, Zhizhong Li and Derek Hoiem</span>
 <br>
    <span class="ban2"> &nbsp; International Journal of Computer Visionn (<b>IJCV</b>), 2019</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://link.springer.com/article/10.1007%2Fs11263-018-1133-z">Paper</a> &nbsp; <a href="https://www.dropbox.com/s/cmjv0dfl7zkzanf/NYUdv2_newgt_cleaned.zip?dl=0">Data</a></span>
 <br>
</p>

<p><br></p>
    
<p>
 <img src="quickview/CVPR_LayoutNet.png" height="200" width="200" align="left"><span> &nbsp; <font size="4">LayoutNet: Reconstructing the 3D Room Layout from a Single RGB Image</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Alex Colburn, Qi Shan and Derek Hoiem</span>
 <br>
    <span class="ban2"> &nbsp; IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2018</span>
 <br>
 <span class="ban2"> &nbsp; <a href="http://openaccess.thecvf.com/content_cvpr_2018/CameraReady/0409.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/LayoutNet">Code</a> &nbsp; <a href="https://towardsdatascience.com/the-10-coolest-papers-from-cvpr-2018-11cb48585a49">The 10 coolest papers from CVPR 2018</a> &nbsp; <a href="https://europe.naverlabs.com/Blog/CVPR-2018-Part-4-3D-Scene-Understanding/">Naver Labs Europe Blog</a> &nbsp; <a href="https://www.jiqizhixin.com/articles/2018-04-04-4">Synced AI Technology & Industry Review (Chinese)</a> &nbsp; <a href="https://shiropen.com/seamless/LayoutNet">Seamless (Japanese)</a> &nbsp; <a href="https://www.youtube.com/watch?v=NBxsfyDaPuk&feature=youtu.be">YouTube 3rd Party Intro</a></span>
 <br>
</p>
    
<p><br></p>
    
<p>
 <img src="quickview/ICCV_3DPRNN.png" height="150" width="200" align="left"><span> &nbsp; <font size="4">3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks</font></span>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Ersin Yumer, Jimei Yang, Duygu Ceylan and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; IEEE International Conference on Computer Vision (<b>ICCV</b>), 2017</span>
 <br>
 <span class="ban2"> &nbsp; <a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Zou_3D-PRNN_Generating_Shape_ICCV_2017_paper.pdf">Paper</a> &nbsp; <a href="https://github.com/zouchuhang/3D-PRNN">Code</a> &nbsp; <a href="https://drive.google.com/file/d/19Jvu6urlDnNAG2gZRf-tObTqyRoeLfpm/view?usp=sharing">Data</a> </span>
 <br>
</p>

<p><br></p>
    
<p>
 <img src="quickview/arxiv_scene.png" height="120" width="200" align="left"><span> &nbsp; <font size="4">Predicting Complete 3D Models of Indoor Scene</font></span>
 <br>
 <br>
    <span class="ban2"> &nbsp; Ruiqi Guo, <b>Chuhang Zou</b> and Derek Hoiem</span>
 <br>
 <span class="ban2"> &nbsp; arxiv, 2015</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://arxiv.org/pdf/1504.02437.pdf">Paper</a> &nbsp; <a href="https://github.com/arron2003/rgbd2full3d">Code</a></span>
 <br>
</p>
 
<p><br></p>
    
<p>
 <img src="quickview/Saliency.png" height="150" width="200" align="left"><span> &nbsp; <font size="4">Salient Object Detection via Fast Iterative Truncated Nuclear Norm Recovery</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; <b>Chuhang Zou</b>, Yao Hu, Deng Cai and Xiaofei He</span>
 <br>
    <span class="ban2"> &nbsp; International Conference on Intelligence Science and Big Data Engineering (ISCIDE), 2013</span>
 <br>
    <span class="ban2"> &nbsp; (<b>Full Oral</b>)
 <br> 
 <span class="ban2"> &nbsp; <a href="https://link.springer.com/chapter/10.1007/978-3-642-42057-3_31">Paper</a></span>
 <br>
</p>
    
<p>
 <img src="quickview/TRJ.jpeg" height="120" width="200" align="left"><span> &nbsp; <font size="4">Extraction and Classification of She Nationality Clothing via Visual Features</font></span>
 <br>
 <br>
 <span class="ban2"> &nbsp; Xiaojun Ding, <b>Chuhang Zou</b>, Jingyu Chen and Fengyuan Zou</span>
 <br>
    <span class="ban2"> &nbsp; Textile Research Journal (TRJ), 2015</span>
 <br>
 <span class="ban2"> &nbsp; <a href="https://journals.sagepub.com/doi/abs/10.1177/0040517515609260?journalCode=trjc">Paper</a></span>
 <br>
</p>
    
    
    
<p>
<br>
</p>

<hr>
</body></html>
